{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "from utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.utils import set_random_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 13\n",
    "set_random_seed(random_seed)\n",
    "rng = np.random.default_rng(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_version = 3\n",
    "root_path = Path('results') / f'dataset_V{dataset_version}'\n",
    "params_path = root_path / 'params.npy'\n",
    "params = np.load(params_path, allow_pickle=True).item()\n",
    "locals().update(params)\n",
    "datasets_path = root_path / 'datasets.npy'\n",
    "datasets = np.load(datasets_path, allow_pickle=True).item()\n",
    "locals().update(datasets)\n",
    "train_annotations_path = root_path / 'train_annotations.csv'\n",
    "val_annotations_path = root_path / 'val_annotations.csv'\n",
    "test_annotations_path = root_path / 'test_annotations.csv'\n",
    "train_annotations = pd.read_csv(train_annotations_path, low_memory=False)\n",
    "val_annotations = pd.read_csv(val_annotations_path, low_memory=False)\n",
    "test_annotations = pd.read_csv(test_annotations_path, low_memory=False)\n",
    "root_path /= 'baseline_methods'\n",
    "root_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels, feat_indices = create_features_dataset_from_annotations(train_annotations, sr=sr, frame_length=frame_length, hop_length=hop_length)\n",
    "test_data, test_labels, _ = create_features_dataset_from_annotations(test_annotations, sr=sr, frame_length=frame_length, hop_length=test_hop_length)\n",
    "class_names = np.array(['NoBee', 'Bee'])\n",
    "train_labels, test_labels = class_names[train_labels.astype(int)], class_names[test_labels.astype(int)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(feat_indices.keys())\n",
    "features_subsets = generate_subsets(features)\n",
    "features_subsets = sorted(features_subsets, key=lambda x: len(x))[1:]\n",
    "features_columns = pd.MultiIndex.from_tuples([('features', f) for f in feat_indices.keys()])\n",
    "features_df = pd.DataFrame(columns=features_columns, data=np.full((1, len(features)), False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = root_path / 'knn_classification_report/feature_selection_results.csv'\n",
    "if results_path.exists():\n",
    "    results_features = pd.read_csv(results_path, index_col=0, header=[0, 1])\n",
    "    results_features.rename(lambda x: '' if x.startswith('Unnamed') else x, axis=1, level=0, inplace=True)\n",
    "else:\n",
    "    results_path.mkdir(parents=True, exist_ok=True)\n",
    "    results_features = None\n",
    "    \n",
    "# Searching for the best subset of features with KNN\n",
    "seen_combinations = results_features.apply(lambda x: '+'.join(sorted(x.features[x.features].index)), axis=1).tolist() if results_features is not None else []\n",
    "\n",
    "for features_subset in features_subsets:\n",
    "    combination = '+'.join(sorted(features_subset))\n",
    "    if combination in seen_combinations: continue\n",
    "    print(f'Processing features subset: {combination}')\n",
    "    selected_features_indices = np.concatenate([feat_indices[feat_name] for feat_name in features_subset])\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn.fit(train_data[:, selected_features_indices], train_labels)\n",
    "    x_pred = knn.predict(test_data[:, selected_features_indices])\n",
    "\n",
    "    classification_report_results = classification_report(test_labels, x_pred, output_dict=True)\n",
    "    classification_report_results = convert_classification_report_to_df(classification_report_results)\n",
    "    features_ = features_df.copy()\n",
    "    features_.loc[0, ('features', features_subset)] = True\n",
    "    classification_report_results = pd.concat([classification_report_results, features_], axis=1)\n",
    "    \n",
    "    results_features = pd.concat([results_features, classification_report_results], axis=0, ignore_index=True) if results_features is not None else classification_report_results\n",
    "    results_features.round(3).to_csv(results_path)\n",
    "    results_features.to_json(results_path.with_suffix('.json'), orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "sns.scatterplot(data=results_features, x=('Bee', 'f1-score'), y=('NoBee', 'f1-score'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "results_features.Bee.plot.scatter(x='precision', y='recall', marker='.', ax=ax, color='blue', label='Bee')\n",
    "results_features.NoBee.plot.scatter(x='precision', y='recall', marker='+', ax=ax, color='orange', label='NoBee')\n",
    "results_features['macro avg'].plot.scatter(x='precision', y='recall', marker='x', ax=ax, color='green', label='Macro avg')\n",
    "results_features.sort_values(by=('macro avg', 'f1-score'), ascending=False, inplace=True)\n",
    "best_subset_results = results_features.iloc[0].to_frame().T\n",
    "best_subset_results.Bee.plot.scatter(x='precision', y='recall', marker='o', ax=ax, color='red', s=50)\n",
    "best_subset_results.NoBee.plot.scatter(x='precision', y='recall', marker='o', ax=ax, color='red', s=50)\n",
    "best_subset_results['macro avg'].plot.scatter(x='precision', y='recall', marker='o', ax=ax, color='red', s=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_save_path = root_path / 'results_methods.csv'\n",
    "if results_save_path.exists():\n",
    "    results_methods = pd.read_csv(results_save_path, index_col=0, header=[0, 1])\n",
    "    results_methods.rename(lambda x: '' if x.startswith('Unnamed') else x, axis=1, level=0, inplace=True)\n",
    "    results_methods.rename(lambda x: '' if x.startswith('Unnamed') else x, axis=1, level=1, inplace=True)\n",
    "else:\n",
    "    results_methods = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_subset = best_subset_results.features.T.index[best_subset_results.features.values[0]].tolist()\n",
    "print(f'Best subset of features: {best_subset}')\n",
    "selected_features_indices = np.concatenate([feat_indices[feat_name] for feat_name in best_subset])\n",
    "train_data = train_data[:, selected_features_indices]\n",
    "test_data = test_data[:, selected_features_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results_methods is None or 'KNN' not in results_methods.method.values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn.fit(train_data, train_labels)\n",
    "    x_pred = knn.predict(test_data)\n",
    "    classification_report_results = classification_report(test_labels, x_pred, output_dict=True)\n",
    "    classification_report_results = convert_classification_report_to_df(classification_report_results)\n",
    "    classification_report_results['method'] = 'KNN'\n",
    "    results_methods = pd.concat([results_methods, classification_report_results], axis=0, ignore_index=True) if results_methods is not None else classification_report_results\n",
    "    results_methods.round(3).to_csv(results_save_path)\n",
    "    results_methods.to_json(results_save_path.with_suffix('.json'), orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'SVC' not in results_methods.method.values:\n",
    "    from sklearn.svm import SVC\n",
    "    svc = SVC()\n",
    "    svc.fit(train_data, train_labels)\n",
    "    x_pred = svc.predict(test_data)\n",
    "    classification_report_results = classification_report(test_labels, x_pred, output_dict=True)\n",
    "    classification_report_results = convert_classification_report_to_df(classification_report_results)\n",
    "    classification_report_results['method'] = 'SVC'\n",
    "    results_methods = pd.concat([results_methods, classification_report_results], axis=0, ignore_index=True)\n",
    "    results_methods.round(3).to_csv(results_save_path)\n",
    "    results_methods.to_json(results_save_path.with_suffix('.json'), orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'RandomForest' not in results_methods.method.values:\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    dec_tree = RandomForestClassifier()\n",
    "    dec_tree.fit(train_data, train_labels)\n",
    "    x_pred = dec_tree.predict(test_data)\n",
    "    classification_report_results = classification_report(test_labels, x_pred, output_dict=True)\n",
    "    classification_report_results = convert_classification_report_to_df(classification_report_results)\n",
    "    classification_report_results['method'] = 'RandomForest'\n",
    "    results_methods = pd.concat([results_methods, classification_report_results], axis=0, ignore_index=True)\n",
    "    results_methods.round(3).to_csv(results_save_path)\n",
    "    results_methods.to_json(results_save_path.with_suffix('.json'), orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'ExtraTrees' not in results_methods.method.values:\n",
    "    from sklearn.ensemble import ExtraTreesClassifier\n",
    "    ert = ExtraTreesClassifier()\n",
    "    ert.fit(train_data, train_labels)\n",
    "    x_pred = ert.predict(test_data)\n",
    "    classification_report_results = classification_report(test_labels, x_pred, output_dict=True)\n",
    "    classification_report_results = convert_classification_report_to_df(classification_report_results)\n",
    "    classification_report_results['method'] = 'ExtraTrees'\n",
    "    results_methods = pd.concat([results_methods, classification_report_results], axis=0, ignore_index=True)\n",
    "    results_methods.round(3).to_csv(results_save_path)\n",
    "    results_methods.to_json(results_save_path.with_suffix('.json'), orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'GradientBoosting' not in results_methods.method.values:\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    gb = GradientBoostingClassifier()\n",
    "    gb.fit(train_data, train_labels)\n",
    "    x_pred = gb.predict(test_data)\n",
    "    classification_report_results = classification_report(test_labels, x_pred, output_dict=True)\n",
    "    classification_report_results = convert_classification_report_to_df(classification_report_results)\n",
    "    classification_report_results['method'] = 'GradientBoosting'\n",
    "    results_methods = pd.concat([results_methods, classification_report_results], axis=0, ignore_index=True)\n",
    "    results_methods.round(3).to_csv(results_save_path)\n",
    "    results_methods.to_json(results_save_path.with_suffix('.json'), orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'LogisticRegression' not in results_methods.method.values:\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(train_data, train_labels)\n",
    "    x_pred = lr.predict(test_data)\n",
    "    classification_report_results = classification_report(test_labels, x_pred, output_dict=True)\n",
    "    classification_report_results = convert_classification_report_to_df(classification_report_results)\n",
    "    classification_report_results['method'] = 'LogisticRegression'\n",
    "    results_methods = pd.concat([results_methods, classification_report_results], axis=0, ignore_index=True)\n",
    "    results_methods.round(3).to_csv(results_save_path)\n",
    "    results_methods.to_json(results_save_path.with_suffix('.json'), orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'KMeans' not in results_methods.method.values:\n",
    "    from sklearn.cluster import KMeans\n",
    "    clustering = KMeans(n_clusters=2, n_init='auto', random_state=random_seed)\n",
    "    clustering.fit(train_data)\n",
    "    labels_map_0 = {0: 'NoBee', 1: 'Bee'}\n",
    "    labels_map_1 = {1: 'NoBee', 0: 'Bee'}\n",
    "    accuracy_0 = classification_report(train_labels, np.vectorize(labels_map_0.get)(clustering.labels_), output_dict=True)['accuracy']\n",
    "    accuracy_1 = classification_report(train_labels, np.vectorize(labels_map_1.get)(clustering.labels_), output_dict=True)['accuracy']\n",
    "    if accuracy_0 > accuracy_1: labels_map = labels_map_0\n",
    "    else: labels_map = labels_map_1\n",
    "    x_pred = clustering.predict(test_data)\n",
    "    x_pred = np.vectorize(labels_map.get)(x_pred)\n",
    "    classification_report_results = classification_report(test_labels, x_pred, output_dict=True)\n",
    "    classification_report_results = convert_classification_report_to_df(classification_report_results)\n",
    "    classification_report_results['method'] = 'KMeans'\n",
    "    results_methods = pd.concat([results_methods, classification_report_results], axis=0, ignore_index=True)\n",
    "    results_methods.round(3).to_csv(results_save_path)\n",
    "    results_methods.to_json(results_save_path.with_suffix('.json'), orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = root_path / 'NN_model/model.h5'\n",
    "if model_path.exists():\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    history = pd.read_csv(model_path.with_name('history.csv'))\n",
    "else:\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Dropout, BatchNormalization\n",
    "    from keras.callbacks import EarlyStopping\n",
    "    from keras.optimizers import Adadelta\n",
    "    from keras.losses import mean_squared_logarithmic_error\n",
    "    \n",
    "    train_labels = np.vectorize({'NoBee': 0., 'Bee': 1.}.get)(train_labels)\n",
    "    val_data, val_labels, _ = create_features_dataset_from_annotations(val_annotations, sr=sr, frame_length=frame_length, hop_length=test_hop_length)\n",
    "    val_labels = val_labels.astype(float)\n",
    "    val_data = val_data[:, selected_features_indices]\n",
    "    \n",
    "    batch_size = 100\n",
    "    n_epochs = 100\n",
    "    \n",
    "    set_random_seed(random_seed)\n",
    "        \n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, activation='relu', input_shape=(train_data.shape[1],)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(\n",
    "        loss=mean_squared_logarithmic_error,\n",
    "        optimizer=Adadelta(learning_rate=1e-2),\n",
    "        metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
    "    )\n",
    "    history = model.fit(\n",
    "        train_data, train_labels,\n",
    "        batch_size=batch_size,\n",
    "        epochs=n_epochs,\n",
    "        shuffle=True,\n",
    "        validation_data=(val_data, val_labels),\n",
    "        callbacks=[EarlyStopping(patience=10, restore_best_weights=True)]\n",
    "    )\n",
    "    model.save(model_path)\n",
    "    pd.DataFrame(history.history).to_csv(model_path.with_name('history.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'NN' not in results_methods.method.values:\n",
    "    x_pred = model.predict(test_data)\n",
    "    x_pred = (x_pred > 0.5).astype(int)\n",
    "    x_pred = np.vectorize({0: 'NoBee', 1: 'Bee'}.get)(x_pred)\n",
    "    classification_report_results = classification_report(test_labels, x_pred, output_dict=True)\n",
    "    classification_report_results = convert_classification_report_to_df(classification_report_results)\n",
    "    classification_report_results['method'] = 'NN'\n",
    "    results_methods = pd.concat([results_methods, classification_report_results], axis=0, ignore_index=True)\n",
    "    results_methods.round(3).to_csv(results_save_path)\n",
    "    results_methods.to_json(results_save_path.with_suffix('.json'), orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "sns.scatterplot(data=results_methods, x=('Bee', 'f1-score'), y=('NoBee', 'f1-score'), hue='method')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepFisFis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
